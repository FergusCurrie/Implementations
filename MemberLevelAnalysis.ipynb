{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with scoring. Will convert to scripts later \n",
    "\n",
    "\n",
    "- must check that size of y and size of scoring file the same\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This jupyter file has code for all graphs and tables at the member level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import sys \n",
    "job = '4081278'\n",
    "from code.data_processing import get_data\n",
    "y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code.data_processing import get_all_datasets\n",
    "all_datasets = get_all_datasets()\n",
    "datasets = {}\n",
    "for d in all_datasets:\n",
    "    datasets[d] = get_data(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First some kind of measure of how dominant a single classifier is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to deal with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single seed of a task with ypred vs observation \n",
    "def load_scoring(fn : str) -> np.array: \n",
    "    assert('.csv' in fn)\n",
    "    x = pd.read_csv(fn).transpose().to_numpy()\n",
    "    assert(len(x.shape) == 2)\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load each scoring dataframe for each seed\n",
    "def load_all_scoring(path : str) -> np.array: \n",
    "    return np.array([load_scoring(path+'/'+x) for x in os.listdir(path) if '.csv' in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy of a single member vector on y\n",
    "def acc_member(X : np.array) -> np.array: # gets the same X for each m ember \n",
    "    assert(len(X.shape) == 1)\n",
    "    assert(len(y) == len(X))\n",
    "    correct = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == X[i]: # fail here \n",
    "            correct += 1\n",
    "    return correct / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy of a single seed\n",
    "def acc_seed(X : np.array) -> np.array:\n",
    "    assert(len(X.shape) == 2)\n",
    "    r = np.zeros(X.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        r[i] = acc_member(X[i])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy over all seeds\n",
    "def acc_task(X : np.array) -> list: # list not dict for now \n",
    "    assert(len(X.shape) == 3)\n",
    "    r = []\n",
    "    for i in range(X.shape[0]):\n",
    "        r.append(acc_seed(X[i]))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphing functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now loading in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all scores in task  = (30, 5, 207), (# seeds, # members in ensemble, # observatins)\n",
      "Shape for each seed = (5,)\n",
      "Shape for acc over seeds = (30, 5)\n"
     ]
    }
   ],
   "source": [
    "scoring_task_1 = load_all_scoring('results_file/4080640/1/scoring/')\n",
    "print(f'Shape of all scores in task  = {scoring_task_1.shape}, (# seeds, # members in ensemble, # observatins)')\n",
    "acc_task_1 = np.array(acc_task(scoring_task_1))\n",
    "print(f'Shape for each seed = {acc_task_1[0].shape}')\n",
    "print(f'Shape for acc over seeds = {acc_task_1.shape}') # might fail on ragged array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53140097, 0.51690821, 0.51690821, 0.52173913, 0.5410628 ],\n",
       "       [0.52173913, 0.50724638, 0.47342995, 0.46859903, 0.47342995],\n",
       "       [0.44927536, 0.49275362, 0.54589372, 0.51207729, 0.50724638],\n",
       "       [0.52173913, 0.56038647, 0.50724638, 0.47342995, 0.46859903],\n",
       "       [0.50241546, 0.51690821, 0.52173913, 0.55555556, 0.46376812],\n",
       "       [0.47342995, 0.50241546, 0.5410628 , 0.50724638, 0.46859903],\n",
       "       [0.5410628 , 0.52173913, 0.53623188, 0.50724638, 0.57487923],\n",
       "       [0.50724638, 0.51690821, 0.4589372 , 0.53140097, 0.50241546],\n",
       "       [0.46376812, 0.46859903, 0.51690821, 0.48309179, 0.48309179],\n",
       "       [0.51690821, 0.49758454, 0.49758454, 0.48309179, 0.48309179],\n",
       "       [0.57004831, 0.50724638, 0.53623188, 0.55555556, 0.57487923],\n",
       "       [0.39613527, 0.45410628, 0.44444444, 0.40096618, 0.45410628],\n",
       "       [0.48792271, 0.47826087, 0.51690821, 0.49758454, 0.44444444],\n",
       "       [0.51207729, 0.50724638, 0.51690821, 0.55555556, 0.55072464],\n",
       "       [0.5410628 , 0.5410628 , 0.51690821, 0.47342995, 0.49758454],\n",
       "       [0.47342995, 0.52173913, 0.48309179, 0.50241546, 0.45410628],\n",
       "       [0.49758454, 0.49758454, 0.51207729, 0.55072464, 0.57004831],\n",
       "       [0.48792271, 0.49758454, 0.49758454, 0.47342995, 0.49758454],\n",
       "       [0.58937198, 0.53140097, 0.57487923, 0.55072464, 0.60869565],\n",
       "       [0.49275362, 0.52657005, 0.51207729, 0.47342995, 0.48792271],\n",
       "       [0.50724638, 0.48309179, 0.49275362, 0.48309179, 0.52657005],\n",
       "       [0.51690821, 0.51207729, 0.53623188, 0.50724638, 0.51690821],\n",
       "       [0.52657005, 0.5410628 , 0.53623188, 0.56521739, 0.56521739],\n",
       "       [0.54589372, 0.52657005, 0.52173913, 0.53623188, 0.51690821],\n",
       "       [0.52173913, 0.53623188, 0.53140097, 0.50241546, 0.49758454],\n",
       "       [0.45410628, 0.43478261, 0.48309179, 0.4589372 , 0.46376812],\n",
       "       [0.46859903, 0.48309179, 0.53140097, 0.52173913, 0.52173913],\n",
       "       [0.49758454, 0.46376812, 0.49758454, 0.49275362, 0.45410628],\n",
       "       [0.53140097, 0.51690821, 0.53140097, 0.4589372 , 0.49275362],\n",
       "       [0.48792271, 0.5410628 , 0.47342995, 0.54589372, 0.57004831]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_task_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ensemble = pd.read_csv(f'results_file/{job}/1/bagboost_experiment_{job}_task_1_cleveland.csv').describe()\n",
    "#df_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_acc = df_ensemble.loc['mean', 'full_acc']\n",
    "#mean_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at how much each solution dominates\n",
    "Ideally need to find a measure which doesn't depend on ensemble size... variance?\n",
    "I think mean across seeds, variance accross accuracy of members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00052245482197173"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnvr = np.mean(np.var(acc_task_1, axis=1))\n",
    "mnvr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/ferguscurrie/Documents/Uni/Research/Implementations',\n",
       " '/usr/local/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python39.zip',\n",
       " '/usr/local/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9',\n",
       " '/usr/local/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/Users/ferguscurrie/Library/Python/3.9/lib/python/site-packages',\n",
       " '/usr/local/lib/python3.9/site-packages',\n",
       " '/usr/local/opt/python-tk@3.9/libexec',\n",
       " '/usr/local/lib/python3.9/site-packages/IPython/extensions',\n",
       " '/Users/ferguscurrie/.ipython']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing model vs dataset table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def get_ytrain(j, t):\n",
    "    dns = get_all_datasets()\n",
    "    with open(f'results_file/{job}/{task}/{job}_{task}_info.txt') as f:\n",
    "        data = f.read()\n",
    "    for dn in dns:\n",
    "        if dn in data:\n",
    "            X, y = get_data(dn)\n",
    "            seed = 169 * int(task)\n",
    "            _, _, y_train, _ = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y)\n",
    "            return y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_all_tasks = []\n",
    "max_nmembers = -1\n",
    "for task in os.listdir(f'results_file/{job}'):\n",
    "    # Not all done\n",
    "    if len(os.listdir(f'results_file/{job}/{task}')) <= 1:\n",
    "        continue \n",
    "    # Otherwise load it \n",
    "    scoring_task = load_all_scoring(f'results_file/{job}/{task}/scoring/') # (# seeds, # members in ensemble, # observatins)\n",
    "    y = get_ytrain(job, task)\n",
    "    acc_taskx = np.array(acc_task(scoring_task))\n",
    "    acc_all_tasks.append(acc_taskx)\n",
    "    if acc_taskx.shape[1] > max_nmembers:\n",
    "        max_nmembers = acc_taskx.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
