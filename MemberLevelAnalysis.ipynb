{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with scoring. Will convert to scripts later \n",
    "\n",
    "\n",
    "- must check that size of y and size of scoring file the same\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This jupyter file has code for all graphs and tables at the member level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import sys \n",
    "job = '4082125'\n",
    "from code.data_processing import get_data\n",
    "y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code.data_processing import get_all_datasets\n",
    "all_datasets = get_all_datasets()\n",
    "datasets = {}\n",
    "for d in all_datasets:\n",
    "    datasets[d] = get_data(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First some kind of measure of how dominant a single classifier is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to deal with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single seed of a task with ypred vs observation \n",
    "def load_scoring(fn : str) -> np.array: \n",
    "    assert('.csv' in fn)\n",
    "    x = pd.read_csv(fn).transpose().to_numpy()\n",
    "    assert(len(x.shape) == 2)\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load each scoring dataframe for each seed\n",
    "def load_all_scoring(path : str, training=True) -> np.array: \n",
    "    F = []\n",
    "    for x in os.listdir(path):\n",
    "        if (not '.csv' in x):\n",
    "            continue \n",
    "        if training:\n",
    "            if (not 'training' in x):\n",
    "                continue\n",
    "        else:\n",
    "            if (not 'test' in x):\n",
    "                continue\n",
    "        F.append(load_scoring(path+'/'+x))\n",
    "                \n",
    "    return np.array(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy of a single member vector on y\n",
    "def acc_member(X : np.array) -> np.array: # gets the same X for each m ember \n",
    "    assert(len(X.shape) == 1)\n",
    "    assert(len(y) == len(X))\n",
    "    \n",
    "    correct = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == X[i]: # fail here \n",
    "            correct += 1\n",
    "    return correct / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy of a single seed\n",
    "def acc_seed(X : np.array) -> np.array:\n",
    "    assert(len(X.shape) == 2)\n",
    "    r = np.zeros(X.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        r[i] = acc_member(X[i])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy over all seeds\n",
    "def acc_task(X : np.array) -> list: # list not dict for now \n",
    "    assert(len(X.shape) == 3)\n",
    "    r = []\n",
    "    for i in range(X.shape[0]):\n",
    "        r.append(acc_seed(X[i]))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphing functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at how much each solution dominates\n",
    "Ideally need to find a measure which doesn't depend on ensemble size... variance?\n",
    "I think mean across seeds, variance accross accuracy of members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing model vs dataset table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def get_ytrain(j, t, training=True):\n",
    "    dns = get_all_datasets()\n",
    "    with open(f'results_file/{job}/{task}/{job}_{task}_info.txt') as f:\n",
    "        data = f.read()\n",
    "    for dn in dns:\n",
    "        if dn in data:\n",
    "            X, y = get_data(dn)\n",
    "            seed = 169 * int(task)\n",
    "            _, _, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y)\n",
    "            if training:\n",
    "                return y_train\n",
    "            else:\n",
    "                return y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_all_tasks = []\n",
    "max_nmembers = -1\n",
    "for task in os.listdir(f'results_file/{job}'):\n",
    "    # Not all done\n",
    "    if len(os.listdir(f'results_file/{job}/{task}')) <= 1:\n",
    "        continue \n",
    "    # Otherwise load it \n",
    "    scoring_task = load_all_scoring(f'results_file/{job}/{task}/scoring/', training=False) # (# seeds, # members in ensemble, # observatins)\n",
    "    # print(scoring_task) - favouring 1/0s? \n",
    "    y = get_ytrain(job, task, training=False)\n",
    "    acc_taskx = np.array(acc_task(scoring_task))\n",
    "    acc_all_tasks.append(acc_taskx)\n",
    "    if acc_taskx.shape[1] > max_nmembers:\n",
    "        max_nmembers = acc_taskx.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
